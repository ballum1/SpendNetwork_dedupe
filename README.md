## Description of folders and files

Included here are scripts and settings/training files for dedupe. I have not included the csvs (the inputs csvs and the results csvs) here (and have set git to ignore them) so as not to make them visible to the public.
However I have saved all of the csvs that I used in the SpendNetwork basecamp (and will give some thoughts on the results).

### record_linkage

Contains scripts and settings for 1 to 1 dedupe matching between two different csv files.
Currently set up to match between a csv of unmatched supplier strings from usm3 and a master csv of suppliers from the supplier table

- spend_network_linkage_example.py contains the script to carry out the matching.

- data_matching_learned_settings and data_matching_training.json contain the model settings for the trained matcher.

- output_cleanup.py is a seaparate script to clean up the output csvs generated by spend_network_linkage_example.py

spend_network_linkage_example.py requires two csvs to run with a matching field name, and generates an output csv with the results.

Notes:
- The matcher has been trained by me giving manual inputs of roughly 40 positive and negative pair matches using the AB sample datasets.
If you want to see how this training works then removing the settings and json files (or changing their paths) will commence training of a new model when spend_network_linkage_example.py is run.
- Dedupe will only work if two field names in the two csvs are the same (currently using "sss" as supplier name).
- I am working on another script (gazetteer) that will make it possible to e.g. match many unmatched supplier strings from usm3 to a single supplier (i.e. not strictly 1:1).



### single_file_cluster

Contains scripts, settings for deduplicating (clustering) a single file (e.g. a list of unmatched suppliers).

The deduplicator has been trained using manual inputs of roughly 40 positive and negative matches from a 10k sample of usm3.
Running on roughly 10k rows took me a few minutes.

I did also run this on the entirety of usm3, which took several hours.

Testing so far implies that roughly 10% of usm3 will be clustered together.

### gazetteer

This script (named "gazetteer" as this is what dedupe calls it) matches between records from different files, but without the 1:1 constraint of record_linkage.
Set up to cluster one or more (up to five) unmatched supplier strings to a single supplier from the supplier table.

(I think it can kind of be thought of like running single_file_cluster on unmatched suppliers to cluster any together before then matching to supplier names)

Currently uses the same settings and training for matching as is used in result_linkage.

## Setup (from dedupe repo)
We recommend using [virtualenv](http://virtualenv.readthedocs.org/en/latest/virtualenv.html) and [virtualenvwrapper](http://virtualenvwrapper.readthedocs.org/en/latest/install.html) for working in a virtualized development environment. [Read how to set up virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/).

Once you have virtualenvwrapper set up,

```bash
mkvirtualenv dedupe-examples
pip install -r requirements.txt
```

## Getting the csvs for input:

Csv samples (e.g. beginning "AB") can be obtained from the DB using the following SQL queries:

```
SELECT
  supplier_name AS sss,
  supplier_id,
  addr_postcode
FROM public.supplier WHERE (supplier_name LIKE 'AB%') AND (supplier_id IS NOT NULL);
```
and

```
SELECT
  sss
FROM public.usm3
WHERE (sss LIKE 'AB%') AND (sid IS NULL);
```
I have saved the csv files I used in basecamp.


## Future Improvements:

- Add postcodes where possible and retrain the matcher accordingly.
- Incorporate blocking from the dedupe repo to make script more suitable for running on big data.
- Alternate uses (tender data, with multiple fields to match)?